{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Play mode >> 1.Text 2.Graphic: 2\n",
      "--------------- \n",
      "episode: 1\n",
      "First Turn: You\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "abstract",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7a4497039204>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---- BOARD ----\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPLAYER\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOPPONENT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyProject/gym-tictactoe/tictactoe_env.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# -------------------- 뷰어 생성 --------------------- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;31m# 캔버스 역할의 뷰어 초기화 가로 세로 300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;31m# 가로 세로 선 생성 (시작점좌표, 끝점좌표), 색정하기 (r, g, b)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mline_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, display)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         self.window = pyglet.window.Window(\n\u001b[0;32m---> 57\u001b[0;31m             width=width, height=height, display=display)\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_closed_by_user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    502\u001b[0m                 None]:\n\u001b[1;32m    503\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m                     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mNoSuchConfigException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pyglet/canvas/base.py\u001b[0m in \u001b[0;36mget_default_screen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mScreen\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         '''\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_screens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pyglet/canvas/base.py\u001b[0m in \u001b[0;36mget_screens\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mScreen\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         '''\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'abstract'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: abstract"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from tictactoe_env import TicTacToeEnv\n",
    "import numpy as np\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "\n",
    "PLAYER = 0\n",
    "OPPONENT = 1\n",
    "MARK_O = 2\n",
    "N, W, Q, P = 0, 1, 2, 3\n",
    "EPISODE = 5\n",
    "\n",
    "\n",
    "class ZeroTree(object):\n",
    "    def __init__(self):\n",
    "        self._load_data()\n",
    "        self.node_memory = deque(maxlen=len(self.state_memory))\n",
    "        self.tree_memory = defaultdict(lambda: 0)\n",
    "        self._make_tree()\n",
    "\n",
    "        # hyperparameter\n",
    "        self.epsilon = 0.25\n",
    "        self.alpha = 3\n",
    "\n",
    "        self.state_data = deque(maxlen=len(self.tree_memory))\n",
    "        self.pi_data = deque(maxlen=len(self.tree_memory))\n",
    "        self._cal_pi()\n",
    "\n",
    "    # 로드할 데이터\n",
    "    def _load_data(self):\n",
    "        self.state_memory = np.load('data/state_memory_30000.npy')\n",
    "        self.edge_memory = np.load('data/edge_memory_30000.npy')\n",
    "\n",
    "    def _make_tree(self):\n",
    "        for v in self.state_memory:\n",
    "            v_tuple = tuple(v)\n",
    "            self.node_memory.append(v_tuple)\n",
    "        tree_tmp = list(zip(self.node_memory, self.edge_memory))\n",
    "        for v in tree_tmp:\n",
    "            self.tree_memory[v[0]] += v[1]\n",
    "\n",
    "    def _cal_pi(self):\n",
    "        for k, v in self.tree_memory.items():\n",
    "            tmp = []\n",
    "            visit_count = []\n",
    "            self.state_data.append(k)\n",
    "            for r in range(3):\n",
    "                for c in range(3):\n",
    "                    visit_count.append(v[r][c][0])\n",
    "            for i in range(9):\n",
    "                tmp.append(visit_count[i] / sum(visit_count))\n",
    "            self.pi_data.append(np.asarray(tmp, 'float').reshape((3, 3)))\n",
    "\n",
    "    def get_pi(self, state):\n",
    "        self.state = state.copy()\n",
    "        board = self.state[PLAYER] + self.state[OPPONENT]\n",
    "        if tuple(state.flatten()) in self.state_data:\n",
    "            i = tuple(self.state.flatten())\n",
    "            j = self.state_data.index(i)\n",
    "            pi = self.pi_data[j]\n",
    "            print('\"zero policy\"')\n",
    "            return pi\n",
    "        else:\n",
    "            empty_loc = np.argwhere(board == 0)\n",
    "            legal_move_n = empty_loc.shape[0]\n",
    "            pi = np.zeros((3, 3))\n",
    "            prob = 1 / legal_move_n\n",
    "            pr = (1 - self.epsilon) * prob + self.epsilon * \\\n",
    "                np.random.dirichlet(self.alpha * np.ones(legal_move_n))\n",
    "            for i in range(legal_move_n):\n",
    "                pi[empty_loc[i][0]][empty_loc[i][1]] = pr[i]\n",
    "            print('\"random policy\"')\n",
    "            return pi\n",
    "\n",
    "\n",
    "# 에이전트 클래스 (실제 플레이 용)\n",
    "class ZeroAgent(object):\n",
    "    def __init__(self):\n",
    "        # 학습한 모델 불러오기\n",
    "        self.model = ZeroTree()\n",
    "\n",
    "        # action space 좌표 공간 구성\n",
    "        self.action_space = self._action_space()\n",
    "\n",
    "        # reset_step member\n",
    "        self.legal_move_n = None\n",
    "        self.empty_loc = None\n",
    "        self.first_turn = None\n",
    "\n",
    "        # reset_episode member\n",
    "        self.action_count = None\n",
    "        self.board = None\n",
    "        self.state = None\n",
    "\n",
    "        # member 초기화\n",
    "        self._reset_step()\n",
    "        self.reset_episode()\n",
    "\n",
    "    def _action_space(self):\n",
    "        action_space = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                action_space.append([i, j])\n",
    "        return np.asarray(action_space)\n",
    "\n",
    "    def _reset_step(self):\n",
    "        self.legal_move_n = 0\n",
    "        self.empty_loc = None\n",
    "\n",
    "    def reset_episode(self):\n",
    "        self.action_count = -1\n",
    "        self.board = None\n",
    "        self.state = None\n",
    "\n",
    "    def select_action(self, state, mode='self'):\n",
    "        if mode == 'self':\n",
    "            self.action_count += 1\n",
    "            user_type = (self.first_turn + self.action_count) % 2\n",
    "            _pi = self.model.get_pi(state)\n",
    "            choice = np.random.choice(9, 1, p=_pi.flatten())\n",
    "            move_target = self.action_space[choice[0]]\n",
    "            action = np.r_[user_type, move_target]\n",
    "            self._reset_step()\n",
    "            return action\n",
    "        elif mode == 'human':\n",
    "            self.action_count += 1\n",
    "            _pi = self.model.get_pi(state)\n",
    "            if self.action_count < 2:\n",
    "                pi_max = np.argwhere(_pi == _pi.max()).tolist()\n",
    "                target = pi_max[np.random.choice(len(pi_max))]\n",
    "                one_hot_pi = np.zeros((3, 3), 'int')\n",
    "                one_hot_pi[target[0]][target[1]] = 1\n",
    "                choice = np.random.choice(\n",
    "                    9, 1, p=one_hot_pi.flatten())\n",
    "            else:\n",
    "                choice = np.random.choice(9, 1, p=_pi.flatten())\n",
    "            move_target = self.action_space[choice[0]]\n",
    "            action = np.r_[OPPONENT, move_target]\n",
    "            self._reset_step()\n",
    "            return action\n",
    "\n",
    "\n",
    "class HumanAgent(object):\n",
    "    def __init__(self):\n",
    "        self.first_turn = None\n",
    "        self.action_space = self._action_space()\n",
    "        self.action_count = -1\n",
    "        self.ai_agent = ZeroAgent()\n",
    "\n",
    "    def reset_episode(self):\n",
    "        self.first_turn = None\n",
    "        self.action_count = -1\n",
    "\n",
    "    def _action_space(self):\n",
    "        action_space = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                action_space.append([i, j])\n",
    "        return np.asarray(action_space)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        self.action_count += 1\n",
    "        if self.first_turn == PLAYER:\n",
    "            if self.action_count % 2 == 0:\n",
    "                print(\"It's your turn!\")\n",
    "                move_target = input(\"1 ~ 9: \")\n",
    "                i = int(move_target) - 1\n",
    "                action = np.r_[PLAYER, self.action_space[i]]\n",
    "                return action\n",
    "            else:\n",
    "                print(\"AI's turn!\")\n",
    "                action = self.ai_agent.select_action(state, mode='human')\n",
    "                return action\n",
    "        else:\n",
    "            if self.action_count % 2 == 0:\n",
    "                print(\"AI's turn!\")\n",
    "                action = self.ai_agent.select_action(state, mode='human')\n",
    "                return action\n",
    "            else:\n",
    "                print(\"It's your turn!\")\n",
    "                move_target = input(\"1 ~ 9: \")\n",
    "                i = int(move_target) - 1\n",
    "                action = np.r_[PLAYER, self.action_space[i]]\n",
    "                return action\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 환경 생성 및 시드 설정\n",
    "    env = TicTacToeEnv()\n",
    "    my_agent = HumanAgent()\n",
    "    # 통계용\n",
    "    result = {1: 0, 0: 0, -1: 0}\n",
    "    # play game\n",
    "    mode = input(\"Play mode >> 1.Text 2.Graphic: \")\n",
    "    if mode == '1':\n",
    "        for e in range(EPISODE):\n",
    "            state = env.reset()\n",
    "            print('-' * 15, '\\nepisode: %d' % (e + 1))\n",
    "            # 선공 정하고 교대로 하기\n",
    "            my_agent.first_turn = ((PLAYER + e) % 2)\n",
    "            # 환경에 알려주기\n",
    "            env.mark_O = my_agent.first_turn\n",
    "            user_type = {PLAYER: 'You', OPPONENT: 'AI'}\n",
    "            print('First Turn: {}'.format(user_type[my_agent.first_turn]))\n",
    "            done = False\n",
    "            while not done:\n",
    "                print(\"---- BOARD ----\")\n",
    "                print(state[PLAYER] + state[OPPONENT] * 2)\n",
    "                # action 선택하기\n",
    "                action = my_agent.select_action(state)\n",
    "                # action 진행\n",
    "                state, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                import time\n",
    "                # 승부난 보드 보기: 내 착수:1, 상대 착수:2\n",
    "                print(\"- FINAL BOARD -\")\n",
    "                print(state[PLAYER] + state[OPPONENT] * 2)\n",
    "                time.sleep(1)\n",
    "                # 결과 dict에 기록\n",
    "                result[reward] += 1\n",
    "                my_agent.reset_episode()\n",
    "                my_agent.ai_agent.reset_episode()\n",
    "    if mode == '2':\n",
    "        for e in range(EPISODE):\n",
    "            state = env.reset()\n",
    "            print('-' * 15, '\\nepisode: %d' % (e + 1))\n",
    "            # 선공 정하고 교대로 하기\n",
    "            my_agent.first_turn = ((PLAYER + e) % 2)\n",
    "            # 환경에 알려주기\n",
    "            env.mark_O = my_agent.first_turn\n",
    "            user_type = {PLAYER: 'You', OPPONENT: 'AI'}\n",
    "            print('First Turn: {}'.format(user_type[my_agent.first_turn]))\n",
    "            done = False\n",
    "            while not done:\n",
    "                env.render()\n",
    "                print(\"---- BOARD ----\")\n",
    "                print(state[PLAYER] + state[OPPONENT] * 2)\n",
    "                # action 선택하기\n",
    "                action = my_agent.select_action(state)\n",
    "                # action 진행\n",
    "                state, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                import time\n",
    "                env.render()\n",
    "                # 승부난 보드 보기: 내 착수:1, 상대 착수:2\n",
    "                print(\"- FINAL BOARD -\")\n",
    "                print(state[PLAYER] + state[OPPONENT] * 2)\n",
    "                time.sleep(1)\n",
    "                # 결과 dict에 기록\n",
    "                result[reward] += 1\n",
    "                my_agent.reset_episode()\n",
    "                my_agent.ai_agent.reset_episode()\n",
    "            env.close()\n",
    "    # 에피소드 통계\n",
    "    print('-' * 15, '\\nWin: %d Lose: %d Draw: %d Winrate: %0.1f%%' %\n",
    "          (result[1], result[-1], result[0], result[1] / EPISODE * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
