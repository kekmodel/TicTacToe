{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Play mode >> 1.Text 2.Graphic: 2\n",
      "--------------- \n",
      "episode: 1\n",
      "First Turn: You\n",
      "---- BOARD ----\n",
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "It's your turn!\n",
      "1 ~ 9: 7\n",
      "---- BOARD ----\n",
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "AI's turn!\n",
      "\"zero policy\"\n",
      "---- BOARD ----\n",
      "[[ 0.  0.  0.]\n",
      " [ 0.  2.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "It's your turn!\n",
      "1 ~ 9: 8\n",
      "---- BOARD ----\n",
      "[[ 0.  0.  0.]\n",
      " [ 0.  2.  0.]\n",
      " [ 1.  1.  0.]]\n",
      "AI's turn!\n",
      "\"zero policy\"\n",
      "---- BOARD ----\n",
      "[[ 0.  0.  0.]\n",
      " [ 0.  2.  0.]\n",
      " [ 1.  1.  2.]]\n",
      "It's your turn!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from tictactoe_env import TicTacToeEnv\n",
    "import numpy as np\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "\n",
    "PLAYER = 0\n",
    "OPPONENT = 1\n",
    "MARK_O = 2\n",
    "N, W, Q, P = 0, 1, 2, 3\n",
    "EPISODE = 5\n",
    "\n",
    "\n",
    "class ZeroTree(object):\n",
    "    def __init__(self):\n",
    "        self._load_data()\n",
    "        self.node_memory = deque(maxlen=len(self.state_memory))\n",
    "        self.tree_memory = defaultdict(lambda: 0)\n",
    "        self._make_tree()\n",
    "\n",
    "        # hyperparameter\n",
    "        self.epsilon = 0.25\n",
    "        self.alpha = 3\n",
    "\n",
    "        self.state_data = deque(maxlen=len(self.tree_memory))\n",
    "        self.pi_data = deque(maxlen=len(self.tree_memory))\n",
    "        self._cal_pi()\n",
    "\n",
    "    # 로드할 데이터\n",
    "    def _load_data(self):\n",
    "        self.state_memory = np.load('data/state_memory_30000.npy')\n",
    "        self.edge_memory = np.load('data/edge_memory_30000.npy')\n",
    "\n",
    "    def _make_tree(self):\n",
    "        for v in self.state_memory:\n",
    "            v_tuple = tuple(v)\n",
    "            self.node_memory.append(v_tuple)\n",
    "        tree_tmp = list(zip(self.node_memory, self.edge_memory))\n",
    "        for v in tree_tmp:\n",
    "            self.tree_memory[v[0]] += v[1]\n",
    "\n",
    "    def _cal_pi(self):\n",
    "        for k, v in self.tree_memory.items():\n",
    "            tmp = []\n",
    "            visit_count = []\n",
    "            self.state_data.append(k)\n",
    "            for r in range(3):\n",
    "                for c in range(3):\n",
    "                    visit_count.append(v[r][c][0])\n",
    "            for i in range(9):\n",
    "                tmp.append(visit_count[i] / sum(visit_count))\n",
    "            self.pi_data.append(np.asarray(tmp, 'float').reshape((3, 3)))\n",
    "\n",
    "    def get_pi(self, state):\n",
    "        self.state = state.copy()\n",
    "        board = self.state[PLAYER] + self.state[OPPONENT]\n",
    "        if tuple(state.flatten()) in self.state_data:\n",
    "            i = tuple(self.state.flatten())\n",
    "            j = self.state_data.index(i)\n",
    "            pi = self.pi_data[j]\n",
    "            print('\"zero policy\"')\n",
    "            return pi\n",
    "        else:\n",
    "            empty_loc = np.argwhere(board == 0)\n",
    "            legal_move_n = empty_loc.shape[0]\n",
    "            pi = np.zeros((3, 3))\n",
    "            prob = 1 / legal_move_n\n",
    "            pr = (1 - self.epsilon) * prob + self.epsilon * \\\n",
    "                np.random.dirichlet(self.alpha * np.ones(legal_move_n))\n",
    "            for i in range(legal_move_n):\n",
    "                pi[empty_loc[i][0]][empty_loc[i][1]] = pr[i]\n",
    "            print('\"random policy\"')\n",
    "            return pi\n",
    "\n",
    "\n",
    "# 에이전트 클래스 (실제 플레이 용)\n",
    "class ZeroAgent(object):\n",
    "    def __init__(self):\n",
    "        # 학습한 모델 불러오기\n",
    "        self.model = ZeroTree()\n",
    "\n",
    "        # action space 좌표 공간 구성\n",
    "        self.action_space = self._action_space()\n",
    "\n",
    "        # reset_step member\n",
    "        self.legal_move_n = None\n",
    "        self.empty_loc = None\n",
    "        self.first_turn = None\n",
    "\n",
    "        # reset_episode member\n",
    "        self.action_count = None\n",
    "        self.board = None\n",
    "        self.state = None\n",
    "\n",
    "        # member 초기화\n",
    "        self._reset_step()\n",
    "        self.reset_episode()\n",
    "\n",
    "    def _action_space(self):\n",
    "        action_space = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                action_space.append([i, j])\n",
    "        return np.asarray(action_space)\n",
    "\n",
    "    def _reset_step(self):\n",
    "        self.legal_move_n = 0\n",
    "        self.empty_loc = None\n",
    "\n",
    "    def reset_episode(self):\n",
    "        self.action_count = -1\n",
    "        self.board = None\n",
    "        self.state = None\n",
    "\n",
    "    def select_action(self, state, mode='self'):\n",
    "        if mode == 'self':\n",
    "            self.action_count += 1\n",
    "            user_type = (self.first_turn + self.action_count) % 2\n",
    "            _pi = self.model.get_pi(state)\n",
    "            choice = np.random.choice(9, 1, p=_pi.flatten())\n",
    "            move_target = self.action_space[choice[0]]\n",
    "            action = np.r_[user_type, move_target]\n",
    "            self._reset_step()\n",
    "            return action\n",
    "        elif mode == 'human':\n",
    "            self.action_count += 1\n",
    "            _pi = self.model.get_pi(state)\n",
    "            if self.action_count < 2:\n",
    "                pi_max = np.argwhere(_pi == _pi.max()).tolist()\n",
    "                target = pi_max[np.random.choice(len(pi_max))]\n",
    "                one_hot_pi = np.zeros((3, 3), 'int')\n",
    "                one_hot_pi[target[0]][target[1]] = 1\n",
    "                choice = np.random.choice(\n",
    "                    9, 1, p=one_hot_pi.flatten())\n",
    "            else:\n",
    "                choice = np.random.choice(9, 1, p=_pi.flatten())\n",
    "            move_target = self.action_space[choice[0]]\n",
    "            action = np.r_[OPPONENT, move_target]\n",
    "            self._reset_step()\n",
    "            return action\n",
    "\n",
    "\n",
    "class HumanAgent(object):\n",
    "    def __init__(self):\n",
    "        self.first_turn = None\n",
    "        self.action_space = self._action_space()\n",
    "        self.action_count = -1\n",
    "        self.ai_agent = ZeroAgent()\n",
    "\n",
    "    def reset_episode(self):\n",
    "        self.first_turn = None\n",
    "        self.action_count = -1\n",
    "\n",
    "    def _action_space(self):\n",
    "        action_space = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                action_space.append([i, j])\n",
    "        return np.asarray(action_space)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        self.action_count += 1\n",
    "        if self.first_turn == PLAYER:\n",
    "            if self.action_count % 2 == 0:\n",
    "                print(\"It's your turn!\")\n",
    "                move_target = input(\"1 ~ 9: \")\n",
    "                i = int(move_target) - 1\n",
    "                action = np.r_[PLAYER, self.action_space[i]]\n",
    "                return action\n",
    "            else:\n",
    "                print(\"AI's turn!\")\n",
    "                action = self.ai_agent.select_action(state, mode='human')\n",
    "                return action\n",
    "        else:\n",
    "            if self.action_count % 2 == 0:\n",
    "                print(\"AI's turn!\")\n",
    "                action = self.ai_agent.select_action(state, mode='human')\n",
    "                return action\n",
    "            else:\n",
    "                print(\"It's your turn!\")\n",
    "                move_target = input(\"1 ~ 9: \")\n",
    "                i = int(move_target) - 1\n",
    "                action = np.r_[PLAYER, self.action_space[i]]\n",
    "                return action\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 환경 생성 및 시드 설정\n",
    "    env = TicTacToeEnv()\n",
    "    my_agent = HumanAgent()\n",
    "    # 통계용\n",
    "    result = {1: 0, 0: 0, -1: 0}\n",
    "    # play game\n",
    "    mode = input(\"Play mode >> 1.Text 2.Graphic: \")\n",
    "    if mode == '1':\n",
    "        for e in range(EPISODE):\n",
    "            state = env.reset()\n",
    "            print('-' * 15, '\\nepisode: %d' % (e + 1))\n",
    "            # 선공 정하고 교대로 하기\n",
    "            my_agent.first_turn = ((PLAYER + e) % 2)\n",
    "            # 환경에 알려주기\n",
    "            env.mark_O = my_agent.first_turn\n",
    "            user_type = {PLAYER: 'You', OPPONENT: 'AI'}\n",
    "            print('First Turn: {}'.format(user_type[my_agent.first_turn]))\n",
    "            done = False\n",
    "            while not done:\n",
    "                print(\"---- BOARD ----\")\n",
    "                print(state[PLAYER] + state[OPPONENT] * 2)\n",
    "                # action 선택하기\n",
    "                action = my_agent.select_action(state)\n",
    "                # action 진행\n",
    "                state, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                import time\n",
    "                # 승부난 보드 보기: 내 착수:1, 상대 착수:2\n",
    "                print(\"- FINAL BOARD -\")\n",
    "                print(state[PLAYER] + state[OPPONENT] * 2)\n",
    "                time.sleep(1)\n",
    "                # 결과 dict에 기록\n",
    "                result[reward] += 1\n",
    "                my_agent.reset_episode()\n",
    "                my_agent.ai_agent.reset_episode()\n",
    "    if mode == '2':\n",
    "        for e in range(EPISODE):\n",
    "            state = env.reset()\n",
    "            print('-' * 15, '\\nepisode: %d' % (e + 1))\n",
    "            # 선공 정하고 교대로 하기\n",
    "            my_agent.first_turn = ((PLAYER + e) % 2)\n",
    "            # 환경에 알려주기\n",
    "            env.mark_O = my_agent.first_turn\n",
    "            user_type = {PLAYER: 'You', OPPONENT: 'AI'}\n",
    "            print('First Turn: {}'.format(user_type[my_agent.first_turn]))\n",
    "            done = False\n",
    "            while not done:\n",
    "                env.render()\n",
    "                print(\"---- BOARD ----\")\n",
    "                print(state[PLAYER] + state[OPPONENT] * 2)\n",
    "                # action 선택하기\n",
    "                action = my_agent.select_action(state)\n",
    "                # action 진행\n",
    "                state, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                import time\n",
    "                env.render()\n",
    "                # 승부난 보드 보기: 내 착수:1, 상대 착수:2\n",
    "                print(\"- FINAL BOARD -\")\n",
    "                print(state[PLAYER] + state[OPPONENT] * 2)\n",
    "                time.sleep(1)\n",
    "                # 결과 dict에 기록\n",
    "                result[reward] += 1\n",
    "                my_agent.reset_episode()\n",
    "                my_agent.ai_agent.reset_episode()\n",
    "            env.close()\n",
    "    # 에피소드 통계\n",
    "    print('-' * 15, '\\nWin: %d Lose: %d Draw: %d Winrate: %0.1f%%' %\n",
    "          (result[1], result[-1], result[0], result[1] / EPISODE * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
